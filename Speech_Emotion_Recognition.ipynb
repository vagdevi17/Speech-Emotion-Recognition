{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vagdevi17/Speech-Emotion-Recognition/blob/main/Speech_Emotion_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Je6iFPSfzGHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "YveqkB65gq-s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uA6RM69Eit6-"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from matplotlib.pyplot import specgram\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import regularizers\n",
        "import os\n",
        "import glob\n",
        "import pandas as pd\n",
        "import IPython.display as ipd\n",
        "import plotly.express as px\n",
        "import scipy.io.wavfile\n",
        "import sys\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "WT9X2WRqgveN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RAV = '/content/drive/MyDrive/archive/audio_speech_actors_01-24/'\n",
        "dir_list = os.listdir(RAV)\n",
        "\n",
        "emotion = []\n",
        "gender = []\n",
        "path = []\n",
        "feature = []\n",
        "for i in dir_list:\n",
        "    fname = os.listdir(RAV + i)\n",
        "    for f in fname:\n",
        "        part = f.split('.')[0].split('-')\n",
        "        emotion.append(int(part[2]))\n",
        "        temp = int(part[6])\n",
        "        if temp%2 == 0:\n",
        "            temp = \"female\"\n",
        "        else:\n",
        "            temp = \"male\"\n",
        "        gender.append(temp)\n",
        "        path.append(RAV + i + '/' + f)\n",
        "        RAV_df = pd.DataFrame(emotion)\n",
        "RAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\n",
        "RAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\n",
        "RAV_df.columns = ['gender','emotion']\n",
        "RAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\n",
        "RAV_df['source'] = 'RAVDESS'\n",
        "RAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n",
        "RAV_df = RAV_df.drop(['gender'], axis=1)\n",
        "RAV_df.labels.value_counts()"
      ],
      "metadata": {
        "id": "CRQyRhx-j2iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(RAV_df.head())\n",
        "display(RAV_df.describe())"
      ],
      "metadata": {
        "id": "pW4MY7oK5dWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Visualization**"
      ],
      "metadata": {
        "id": "nTesaSZ1g0we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "px_fig = px.histogram(RAV_df, x='emotion', color='emotion', marginal='box',\n",
        "                      title='Emotion Count')\n",
        "px_fig.update_layout(bargap=0.2)\n",
        "px_fig.show()\n",
        "\n",
        "px_fig = px.histogram(RAV_df, x='labels', color='emotion', marginal='box',\n",
        "                      title='Label Count')\n",
        "px_fig.update_layout(bargap=0.2)\n",
        "px_fig.show()"
      ],
      "metadata": {
        "id": "cdWwKgXH5iha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_waveplot(data, sr, e):\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.title('Waveplot for audio with {} emotion'.format(e), size=15)\n",
        "    librosa.display.waveshow(data, sr=sr)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0mUo_SIj5nKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_spectrogram(data, sr, e):\n",
        "    # stft function converts the data into short term fourier transform\n",
        "    X = librosa.stft(data)\n",
        "    Xdb = librosa.amplitude_to_db(abs(X))\n",
        "    plt.figure(figsize=(12, 3))\n",
        "    plt.title('Spectrogram for audio with {} emotion'.format(e), size=15)\n",
        "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz')\n",
        "    #librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='log')\n",
        "    plt.colorbar()"
      ],
      "metadata": {
        "id": "zyhmKXzM5rXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion='fear'\n",
        "path = np.array(RAV_df.path[RAV_df.emotion==emotion])[1]\n",
        "data, sampling_rate = librosa.load(path)\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "ipd.Audio(path)"
      ],
      "metadata": {
        "id": "h5H5WAWT5t16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Augmentation**"
      ],
      "metadata": {
        "id": "-yh-EGebg93D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def noise(data):\n",
        "    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n",
        "    data = data + noise_amp*np.random.normal(size=data.shape[0])\n",
        "    return data\n",
        "\n",
        "def stretch(data, rate=0.8):\n",
        "    return librosa.effects.time_stretch(data, rate = rate)\n",
        "\n",
        "def shift(data):\n",
        "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
        "    return np.roll(data, shift_range)\n",
        "\n",
        "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
        "    return librosa.effects.pitch_shift(data, sr=sampling_rate, n_steps=pitch_factor)\n",
        "\n",
        "# taking any example and checking for techniques.\n",
        "path = np.array(RAV_df.path)[1]\n",
        "data, sample_rate = librosa.load(path)"
      ],
      "metadata": {
        "id": "LchJrdiJ6Q4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=data, sr=sample_rate)\n",
        "ipd.Audio(path)"
      ],
      "metadata": {
        "id": "LjCiZvmt6U0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = noise(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "ipd.Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "_OrFGVTM6YiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = stretch(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "ipd.Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "R0-ioIWY6h7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = shift(data)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "ipd.Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "7KL7wQkS6lX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = pitch(data,sample_rate)\n",
        "plt.figure(figsize=(14,4))\n",
        "librosa.display.waveshow(y=x, sr=sample_rate)\n",
        "ipd.Audio(x, rate=sample_rate)"
      ],
      "metadata": {
        "id": "IZYtmKOG6nyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Extraction :**"
      ],
      "metadata": {
        "id": "2u1TvqW36wJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(data):\n",
        "    # ZCR\n",
        "    result = np.array([])\n",
        "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
        "    result=np.hstack((result, zcr)) # stacking horizontally\n",
        "\n",
        "    # Chroma_stft\n",
        "    stft = np.abs(librosa.stft(data))\n",
        "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, chroma_stft)) # stacking horizontally\n",
        "\n",
        "    # MFCC\n",
        "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mfcc)) # stacking horizontally\n",
        "\n",
        "    # Root Mean Square Value\n",
        "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)\n",
        "    result = np.hstack((result, rms)) # stacking horizontally\n",
        "\n",
        "    # MelSpectogram\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)\n",
        "    result = np.hstack((result, mel)) # stacking horizontally\n",
        "    return result\n",
        "\n",
        "def get_features(path):\n",
        "    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n",
        "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
        "\n",
        "    # without augmentation\n",
        "    res1 = extract_features(data)\n",
        "    result = np.array(res1)\n",
        "\n",
        "    # data with noise\n",
        "    noise_data = noise(data)\n",
        "    res2 = extract_features(noise_data)\n",
        "    result = np.vstack((result, res2)) # stacking vertically\n",
        "\n",
        "    # data with stretching and pitching\n",
        "    new_data = stretch(data)\n",
        "    data_stretch_pitch = pitch(new_data, sample_rate)\n",
        "    res3 = extract_features(data_stretch_pitch)\n",
        "    result = np.vstack((result, res3)) # stacking vertically\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "N_YXE63x6tB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(RAV_df.path))"
      ],
      "metadata": {
        "id": "mCIxajre9bwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preperation**"
      ],
      "metadata": {
        "id": "UxIIFNi36579"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "X, Y = [], []\n",
        "for path, emotion in tqdm.tqdm(zip(RAV_df.path, RAV_df.emotion),total=len(RAV_df.path)):\n",
        "    feature = get_features(path)\n",
        "    for ele in feature:\n",
        "        X.append(ele)\n",
        "        # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n",
        "        Y.append(emotion)"
      ],
      "metadata": {
        "id": "-eRBlczc7fS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(Y), RAV_df.path.shape"
      ],
      "metadata": {
        "id": "DyczHtW0A0TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Features = pd.DataFrame(X)\n",
        "Features['labels'] = Y\n",
        "Features.to_csv('features.csv', index=False)"
      ],
      "metadata": {
        "id": "sosxX7IwA3vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Features.head())\n",
        "display(Features.describe())"
      ],
      "metadata": {
        "id": "theb5pGyA7By"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = Features.iloc[: ,:-1].values\n",
        "Y = Features['labels'].values"
      ],
      "metadata": {
        "id": "xLe_ugf-A-W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = OneHotEncoder()\n",
        "Y = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()"
      ],
      "metadata": {
        "id": "FXZiIhzMBA2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "QI5JRFqiBDT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "yReINITwBEk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = np.expand_dims(x_train, axis=2)\n",
        "x_test = np.expand_dims(x_test, axis=2)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "metadata": {
        "id": "Ylom_iP6BIk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modelling**"
      ],
      "metadata": {
        "id": "Gw7zjPw57ued"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv1D(32, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=16, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(units=7, activation='softmax'))\n",
        "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RkKy1_JIBNSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "cBgypmNi8YiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=4, min_lr=0.0000001)\n",
        "history=model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_test, y_test), callbacks=[rlrp])"
      ],
      "metadata": {
        "id": "cTlTh_iBBVeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation**"
      ],
      "metadata": {
        "id": "6h4zIxY19DH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")\n",
        "\n",
        "plt.style.use('seaborn-darkgrid')\n",
        "plt.rcParams.update({'font.size': 12})\n",
        "epochs = [i for i in range(50)]\n",
        "fig , ax = plt.subplots(1,2)\n",
        "train_acc = history.history['accuracy']\n",
        "train_loss = history.history['loss']\n",
        "test_acc = history.history['val_accuracy']\n",
        "test_loss = history.history['val_loss']\n",
        "\n",
        "fig.set_size_inches(20,6)\n",
        "ax[0].plot(epochs , train_loss , label = 'Training Loss',marker='o', linewidth=2)\n",
        "ax[0].plot(epochs , test_loss , label = 'Testing Loss',marker='.', linewidth=2)\n",
        "ax[0].set_title('Training & Testing Loss')\n",
        "ax[0].legend()\n",
        "ax[0].set_xlabel(\"Epochs\")\n",
        "\n",
        "ax[1].plot(epochs , train_acc , label = 'Training Accuracy',marker='o', linewidth=2)\n",
        "ax[1].plot(epochs , test_acc , label = 'Testing Accuracy',marker='.', linewidth=2)\n",
        "ax[1].set_title('Training & Testing Accuracy')\n",
        "ax[1].legend()\n",
        "ax[1].set_xlabel(\"Epochs\")\n",
        "\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MbEB0hc4D9zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_test = model.predict(x_test)\n",
        "y_pred = encoder.inverse_transform(pred_test)\n",
        "\n",
        "y_test = encoder.inverse_transform(y_test)"
      ],
      "metadata": {
        "id": "YJSYOcLDEF9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\n",
        "df['Predicted Labels'] = y_pred.flatten()\n",
        "df['Actual Labels'] = y_test.flatten()\n",
        "\n",
        "display(df.head())"
      ],
      "metadata": {
        "id": "Svr2DlqkEJwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize = (12, 10))\n",
        "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
        "sns.heatmap(cm, linecolor='white', cmap='Purples', linewidth=1, annot=True, fmt='')\n",
        "plt.title('Confusion Matrix', size=20)\n",
        "plt.xlabel('Predicted Labels', size=14)\n",
        "plt.ylabel('Actual Labels', size=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h84tCGt-EM00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "NiPImuhkEQKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Live Demo**\n",
        "\n",
        "The file '*.wav' in the next cell is the file that was recorded live using the code in AudioRecoreder notebook found in the repository"
      ],
      "metadata": {
        "id": "5dE6J_zwx9NR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/archive/Actor_01/03-01-01-01-01-01-01.wav'\n",
        "# part = path.split(\"/\")[-1].split('.')[0].split('-')\n",
        "# emotion = {1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'}[int(part[2])]\n",
        "# data, sampling_rate = librosa.load(path)\n",
        "emotion = \"\"\n",
        "create_waveplot(data, sampling_rate, emotion)\n",
        "create_spectrogram(data, sampling_rate, emotion)\n",
        "ipd.Audio(path)"
      ],
      "metadata": {
        "id": "DlykXbs1yaOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_live, Y_live = [], []\n",
        "feature = get_features(path)\n",
        "for ele in feature:\n",
        "    X_live.append(ele)\n",
        "    # appending emotion 3 times as we have made 3 augmentation techniques on each audio file.\n",
        "    Y_live.append(emotion)"
      ],
      "metadata": {
        "id": "kyeUQbmYzf6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Features_live = pd.DataFrame(X_live)\n",
        "x_live = scaler.transform(Features_live)\n",
        "x_live = np.expand_dims(x_live, axis=2)\n",
        "pred_live = model.predict(x_live)\n",
        "y_pred_live = encoder.inverse_transform(pred_live)\n",
        "print(\"Predicted emotion:\", y_pred_live[0][0])\n",
        "# print(\"Ground truth emotion:\", emotion)"
      ],
      "metadata": {
        "id": "vHyuqJnH1A8G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}